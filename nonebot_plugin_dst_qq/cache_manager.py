"""
å¤šçº§ç¼“å­˜ç®¡ç†ç³»ç»Ÿ

ä½¿ç”¨ LocalStore æ’ä»¶å®ç°æ ‡å‡†åŒ–çš„ç¼“å­˜å­˜å‚¨ï¼š
- é…ç½®ç¼“å­˜: é•¿æœŸæœ‰æ•ˆï¼Œå­˜å‚¨ç”¨æˆ·é…ç½®å’Œæ’ä»¶è®¾ç½®
- æ•°æ®ç¼“å­˜: ä¸­æœŸæœ‰æ•ˆï¼Œå­˜å‚¨APIè°ƒç”¨ç»“æœå’ŒæŸ¥è¯¢ç»“æœ  
- å†…å­˜ç¼“å­˜: çŸ­æœŸæœ‰æ•ˆï¼Œå­˜å‚¨é¢‘ç¹è®¿é—®çš„æ•°æ®

ç¼“å­˜ç­–ç•¥ï¼š
- LRU: å†…å­˜ç¼“å­˜ä½¿ç”¨æœ€è¿‘æœ€å°‘ä½¿ç”¨ç­–ç•¥
- TTL: æ‰€æœ‰ç¼“å­˜æ”¯æŒç”Ÿå­˜æ—¶é—´
- åˆ†çº§å­˜å‚¨: å†…å­˜ -> æ–‡ä»¶ç¼“å­˜ -> æºæ•°æ®
"""

import json
import pickle
import asyncio
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Callable, Awaitable
from functools import wraps
from collections import OrderedDict
from nonebot.log import logger

# Lazy import localstore
def _get_localstore():
    """Lazy import and initialization of localstore"""
    try:
        from nonebot import require
        require("nonebot_plugin_localstore")
        import nonebot_plugin_localstore as store
        return store
    except Exception:
        # Fallback to None if localstore fails
        return None


class LRUCache:
    """LRUç¼“å­˜å®ç°"""
    
    def __init__(self, max_size: int = 128):
        self.max_size = max_size
        self.cache: OrderedDict = OrderedDict()
        self.ttl_data: Dict[str, datetime] = {}
    
    def get(self, key: str, default: Any = None) -> Any:
        """è·å–ç¼“å­˜å€¼"""
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if key in self.ttl_data:
            if datetime.now() > self.ttl_data[key]:
                self._remove(key)
                return default
        
        if key in self.cache:
            # ç§»åˆ°æœ«å°¾ï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
            self.cache.move_to_end(key)
            return self.cache[key]
        
        return default
    
    def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -> None:
        """è®¾ç½®ç¼“å­˜å€¼"""
        if key in self.cache:
            # æ›´æ–°ç°æœ‰å€¼
            self.cache.move_to_end(key)
        else:
            # æ£€æŸ¥å®¹é‡é™åˆ¶
            if len(self.cache) >= self.max_size:
                # ç§»é™¤æœ€æ—§çš„é¡¹
                oldest_key = next(iter(self.cache))
                self._remove(oldest_key)
        
        self.cache[key] = value
        
        # è®¾ç½®TTL
        if ttl_seconds:
            self.ttl_data[key] = datetime.now() + timedelta(seconds=ttl_seconds)
    
    def _remove(self, key: str) -> None:
        """ç§»é™¤ç¼“å­˜é¡¹"""
        self.cache.pop(key, None)
        self.ttl_data.pop(key, None)
    
    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.ttl_data.clear()
    
    def size(self) -> int:
        """è·å–ç¼“å­˜å¤§å°"""
        return len(self.cache)
    
    def keys(self) -> List[str]:
        """è·å–æ‰€æœ‰é”®"""
        return list(self.cache.keys())


class CacheManager:
    """å¤šçº§ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, 
                 memory_cache_size: int = 256,
                 default_memory_ttl: int = 300,    # 5åˆ†é’Ÿ
                 default_file_ttl: int = 1800,     # 30åˆ†é’Ÿ
                 default_config_ttl: int = 3600):  # 1å°æ—¶
        
        # åˆå§‹åŒ–å­˜å‚¨ç›®å½•
        store = _get_localstore()
        if store:
            try:
                self.cache_dir = store.get_plugin_cache_dir()
                self.config_dir = store.get_plugin_config_dir()
                self.data_dir = store.get_plugin_data_dir()
            except Exception:
                # Fallback to plugin directory
                plugin_dir = Path(__file__).parent
                self.cache_dir = plugin_dir / "cache"
                self.config_dir = plugin_dir / "config"
                self.data_dir = plugin_dir / "data"
        else:
            # Fallback to plugin directory
            plugin_dir = Path(__file__).parent
            self.cache_dir = plugin_dir / "cache"
            self.config_dir = plugin_dir / "config"
            self.data_dir = plugin_dir / "data"
        
        # åˆ›å»ºå­ç›®å½•
        self.api_cache_dir = self.cache_dir / "api"
        self.db_cache_dir = self.cache_dir / "database" 
        self.config_cache_dir = self.config_dir / "cache"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for directory in [self.api_cache_dir, self.db_cache_dir, self.config_cache_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # å†…å­˜ç¼“å­˜
        self.memory_cache = LRUCache(memory_cache_size)
        
        # ç¼“å­˜é…ç½®
        self.default_memory_ttl = default_memory_ttl
        self.default_file_ttl = default_file_ttl
        self.default_config_ttl = default_config_ttl
        
        # ç¼“å­˜ç»Ÿè®¡
        self.stats = {
            "memory_hits": 0,
            "file_hits": 0,
            "misses": 0,
            "total_requests": 0
        }
        
        logger.info("ğŸ—„ï¸ å¤šçº§ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“ APIç¼“å­˜ç›®å½•: {self.api_cache_dir}")
        logger.info(f"ğŸ“ æ•°æ®åº“ç¼“å­˜ç›®å½•: {self.db_cache_dir}")
        logger.info(f"ğŸ“ é…ç½®ç¼“å­˜ç›®å½•: {self.config_cache_dir}")
    
    def _generate_cache_key(self, prefix: str, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦
        key_data = {
            "prefix": prefix,
            "args": args,
            "kwargs": sorted(kwargs.items())
        }
        
        # ä½¿ç”¨MD5ç”ŸæˆçŸ­é”®å
        key_str = json.dumps(key_data, sort_keys=True, default=str)
        hash_key = hashlib.md5(key_str.encode()).hexdigest()[:16]
        
        return f"{prefix}_{hash_key}"
    
    def _get_file_cache_path(self, cache_type: str, key: str) -> Path:
        """è·å–æ–‡ä»¶ç¼“å­˜è·¯å¾„"""
        if cache_type == "api":
            return self.api_cache_dir / f"{key}.cache"
        elif cache_type == "db":
            return self.db_cache_dir / f"{key}.cache"  
        elif cache_type == "config":
            return self.config_cache_dir / f"{key}.cache"
        else:
            return self.cache_dir / f"{key}.cache"
    
    async def get(self, 
                  cache_type: str,
                  key: str, 
                  default: Any = None,
                  memory_ttl: Optional[int] = None,
                  file_ttl: Optional[int] = None) -> Any:
        """
        è·å–ç¼“å­˜æ•°æ®
        
        Args:
            cache_type: ç¼“å­˜ç±»å‹ (api/db/config)
            key: ç¼“å­˜é”®
            default: é»˜è®¤å€¼
            memory_ttl: å†…å­˜ç¼“å­˜TTL
            file_ttl: æ–‡ä»¶ç¼“å­˜TTL
        """
        self.stats["total_requests"] += 1
        
        # 1. å°è¯•ä»å†…å­˜ç¼“å­˜è·å–
        memory_value = self.memory_cache.get(key)
        if memory_value is not None:
            self.stats["memory_hits"] += 1
            logger.debug(f"ğŸ§  å†…å­˜ç¼“å­˜å‘½ä¸­: {key}")
            return memory_value
        
        # 2. å°è¯•ä»æ–‡ä»¶ç¼“å­˜è·å–
        file_path = self._get_file_cache_path(cache_type, key)
        if file_path.exists():
            try:
                # è¯»å–ç¼“å­˜æ–‡ä»¶
                with open(file_path, 'rb') as f:
                    cache_data = pickle.load(f)
                
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if cache_data.get("expires_at"):
                    expires_at = datetime.fromisoformat(cache_data["expires_at"])
                    if datetime.now() > expires_at:
                        # è¿‡æœŸï¼Œåˆ é™¤æ–‡ä»¶
                        file_path.unlink()
                        logger.debug(f"ğŸ—‘ï¸ æ–‡ä»¶ç¼“å­˜è¿‡æœŸå·²åˆ é™¤: {key}")
                    else:
                        # æœªè¿‡æœŸï¼ŒåŠ è½½åˆ°å†…å­˜å¹¶è¿”å›
                        value = cache_data["data"]
                        
                        # é‡æ–°åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                        ttl = memory_ttl or self.default_memory_ttl
                        self.memory_cache.set(key, value, ttl)
                        
                        self.stats["file_hits"] += 1
                        logger.debug(f"ğŸ“„ æ–‡ä»¶ç¼“å­˜å‘½ä¸­: {key}")
                        return value
                else:
                    # æ²¡æœ‰è¿‡æœŸæ—¶é—´ï¼Œç›´æ¥ä½¿ç”¨
                    value = cache_data["data"]
                    ttl = memory_ttl or self.default_memory_ttl
                    self.memory_cache.set(key, value, ttl)
                    
                    self.stats["file_hits"] += 1
                    logger.debug(f"ğŸ“„ æ–‡ä»¶ç¼“å­˜å‘½ä¸­ï¼ˆæ— TTLï¼‰: {key}")
                    return value
                    
            except Exception as e:
                logger.warning(f"âš ï¸ è¯»å–æ–‡ä»¶ç¼“å­˜å¤±è´¥: {key}, é”™è¯¯: {e}")
                # å¦‚æœæ˜¯æ¨¡å—å¯¼å…¥é”™è¯¯ï¼ˆé€šå¸¸æ˜¯å› ä¸ºè·¯å¾„å˜æ›´ï¼‰ï¼Œåˆ™æ¸…ç©ºå¯¹åº”ç±»å‹çš„æ‰€æœ‰ç¼“å­˜
                if "No module named" in str(e):
                    logger.warning(f"ğŸ§¹ æ£€æµ‹åˆ°æ¨¡å—è·¯å¾„å˜æ›´ï¼Œæ¸…ç©º {cache_type} ç±»å‹ç¼“å­˜")
                    await self._clear_cache_type_silent(cache_type)
                else:
                    # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
                    try:
                        file_path.unlink()
                    except:
                        pass
        
        # 3. ç¼“å­˜æœªå‘½ä¸­
        self.stats["misses"] += 1
        logger.debug(f"âŒ ç¼“å­˜æœªå‘½ä¸­: {key}")
        return default
    
    async def set(self, 
                  cache_type: str,
                  key: str, 
                  value: Any,
                  memory_ttl: Optional[int] = None,
                  file_ttl: Optional[int] = None,
                  persist_to_file: bool = True) -> None:
        """
        è®¾ç½®ç¼“å­˜æ•°æ®
        
        Args:
            cache_type: ç¼“å­˜ç±»å‹ (api/db/config)
            key: ç¼“å­˜é”®
            value: ç¼“å­˜å€¼
            memory_ttl: å†…å­˜ç¼“å­˜TTL
            file_ttl: æ–‡ä»¶ç¼“å­˜TTL  
            persist_to_file: æ˜¯å¦æŒä¹…åŒ–åˆ°æ–‡ä»¶
        """
        # 1. è®¾ç½®å†…å­˜ç¼“å­˜
        ttl = memory_ttl or self.default_memory_ttl
        self.memory_cache.set(key, value, ttl)
        logger.debug(f"ğŸ§  å†…å­˜ç¼“å­˜å·²è®¾ç½®: {key}, TTL: {ttl}s")
        
        # 2. è®¾ç½®æ–‡ä»¶ç¼“å­˜
        if persist_to_file:
            try:
                file_path = self._get_file_cache_path(cache_type, key)
                
                # å‡†å¤‡ç¼“å­˜æ•°æ®
                cache_data = {"data": value}
                
                # è®¾ç½®è¿‡æœŸæ—¶é—´
                if file_ttl:
                    expires_at = datetime.now() + timedelta(seconds=file_ttl)
                    cache_data["expires_at"] = expires_at.isoformat()
                elif cache_type == "config":
                    expires_at = datetime.now() + timedelta(seconds=self.default_config_ttl)
                    cache_data["expires_at"] = expires_at.isoformat()
                elif cache_type in ("api", "db"):
                    expires_at = datetime.now() + timedelta(seconds=self.default_file_ttl)
                    cache_data["expires_at"] = expires_at.isoformat()
                
                # å†™å…¥æ–‡ä»¶
                with open(file_path, 'wb') as f:
                    pickle.dump(cache_data, f)
                
                logger.debug(f"ğŸ“„ æ–‡ä»¶ç¼“å­˜å·²è®¾ç½®: {key}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ è®¾ç½®æ–‡ä»¶ç¼“å­˜å¤±è´¥: {key}, é”™è¯¯: {e}")
    
    async def delete(self, cache_type: str, key: str) -> None:
        """åˆ é™¤ç¼“å­˜"""
        # åˆ é™¤å†…å­˜ç¼“å­˜
        self.memory_cache._remove(key)
        
        # åˆ é™¤æ–‡ä»¶ç¼“å­˜
        file_path = self._get_file_cache_path(cache_type, key)
        if file_path.exists():
            try:
                file_path.unlink()
                logger.debug(f"ğŸ—‘ï¸ ç¼“å­˜å·²åˆ é™¤: {key}")
            except Exception as e:
                logger.warning(f"âš ï¸ åˆ é™¤æ–‡ä»¶ç¼“å­˜å¤±è´¥: {key}, é”™è¯¯: {e}")
    
    async def _clear_cache_type_silent(self, cache_type: str) -> None:
        """é™é»˜æ¸…ç©ºæŒ‡å®šç±»å‹çš„ç¼“å­˜ï¼ˆç”¨äºé”™è¯¯æ¢å¤ï¼‰"""
        try:
            # æ¸…ç©ºæŒ‡å®šç±»å‹çš„æ–‡ä»¶ç¼“å­˜
            cache_dir = self._get_file_cache_path(cache_type, "").parent
            for cache_file in cache_dir.glob("*.cache"):
                try:
                    cache_file.unlink()
                except:
                    pass  # é™é»˜å¿½ç•¥é”™è¯¯
        except:
            pass  # é™é»˜å¿½ç•¥æ‰€æœ‰é”™è¯¯
    
    async def clear(self, cache_type: Optional[str] = None) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        if cache_type is None:
            # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
            self.memory_cache.clear()
            
            # æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶ç¼“å­˜ç›®å½•
            for directory in [self.api_cache_dir, self.db_cache_dir, self.config_cache_dir]:
                for cache_file in directory.glob("*.cache"):
                    try:
                        cache_file.unlink()
                    except Exception as e:
                        logger.warning(f"âš ï¸ åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {cache_file}, é”™è¯¯: {e}")
            
            logger.info("ğŸ—‘ï¸ æ‰€æœ‰ç¼“å­˜å·²æ¸…ç©º")
        else:
            # æ¸…ç©ºæŒ‡å®šç±»å‹çš„æ–‡ä»¶ç¼“å­˜
            cache_dir = self._get_file_cache_path(cache_type, "").parent
            for cache_file in cache_dir.glob("*.cache"):
                try:
                    cache_file.unlink()
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {cache_file}, é”™è¯¯: {e}")
            
            logger.info(f"ğŸ—‘ï¸ {cache_type} ç±»å‹ç¼“å­˜å·²æ¸…ç©º")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total = self.stats["total_requests"]
        if total > 0:
            hit_rate = (self.stats["memory_hits"] + self.stats["file_hits"]) / total
            memory_hit_rate = self.stats["memory_hits"] / total
            file_hit_rate = self.stats["file_hits"] / total
        else:
            hit_rate = memory_hit_rate = file_hit_rate = 0.0
        
        return {
            **self.stats,
            "hit_rate": hit_rate,
            "memory_hit_rate": memory_hit_rate,
            "file_hit_rate": file_hit_rate,
            "memory_cache_size": self.memory_cache.size(),
            "memory_cache_keys": self.memory_cache.keys()
        }


# ç¼“å­˜è£…é¥°å™¨
def cached(cache_type: str = "api", 
           memory_ttl: Optional[int] = None,
           file_ttl: Optional[int] = None,
           key_generator: Optional[Callable] = None,
           persist_to_file: bool = True):
    """
    ç¼“å­˜è£…é¥°å™¨
    
    Args:
        cache_type: ç¼“å­˜ç±»å‹ (api/db/config)
        memory_ttl: å†…å­˜ç¼“å­˜TTL
        file_ttl: æ–‡ä»¶ç¼“å­˜TTL
        key_generator: è‡ªå®šä¹‰é”®ç”Ÿæˆå™¨
        persist_to_file: æ˜¯å¦æŒä¹…åŒ–åˆ°æ–‡ä»¶
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            if key_generator:
                cache_key = key_generator(*args, **kwargs)
            else:
                cache_key = cache_manager._generate_cache_key(func.__name__, *args, **kwargs)
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = await cache_manager.get(
                cache_type=cache_type,
                key=cache_key,
                memory_ttl=memory_ttl,
                file_ttl=file_ttl
            )
            
            if cached_result is not None:
                return cached_result
            
            # æ‰§è¡ŒåŸå‡½æ•°
            if asyncio.iscoroutinefunction(func):
                result = await func(*args, **kwargs)
            else:
                result = func(*args, **kwargs)
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            if result is not None:  # åªç¼“å­˜éç©ºç»“æœ
                await cache_manager.set(
                    cache_type=cache_type,
                    key=cache_key,
                    value=result,
                    memory_ttl=memory_ttl,
                    file_ttl=file_ttl,
                    persist_to_file=persist_to_file
                )
            
            return result
        
        return wrapper
    return decorator


# åˆ›å»ºå…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
cache_manager = CacheManager()


# ä¾¿æ·å‡½æ•°
async def get_api_cache(key: str, default: Any = None) -> Any:
    """è·å–APIç¼“å­˜"""
    return await cache_manager.get("api", key, default)


async def set_api_cache(key: str, value: Any, ttl: int = 300) -> None:
    """è®¾ç½®APIç¼“å­˜"""
    await cache_manager.set("api", key, value, memory_ttl=ttl, file_ttl=ttl*2)


async def get_db_cache(key: str, default: Any = None) -> Any:
    """è·å–æ•°æ®åº“ç¼“å­˜"""
    return await cache_manager.get("db", key, default)


async def set_db_cache(key: str, value: Any, ttl: int = 600) -> None:
    """è®¾ç½®æ•°æ®åº“ç¼“å­˜"""
    await cache_manager.set("db", key, value, memory_ttl=ttl, file_ttl=ttl*3)


async def get_config_cache(key: str, default: Any = None) -> Any:
    """è·å–é…ç½®ç¼“å­˜"""
    return await cache_manager.get("config", key, default)


async def set_config_cache(key: str, value: Any, ttl: int = 3600) -> None:
    """è®¾ç½®é…ç½®ç¼“å­˜"""
    await cache_manager.set("config", key, value, memory_ttl=ttl, file_ttl=ttl*2)
